{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "导入程序所需要的包",
   "id": "fa88f541e11dd526"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T07:41:10.938849Z",
     "start_time": "2025-04-25T07:41:10.927870Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31,
   "source": [
    "import torch\n",
    "import torch.utils.data as DataSet\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "# from torchsummary import summary\n",
    "import string\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ],
   "id": "ee482e60477f976d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "读入，进行数据预处理并展示数据",
   "id": "f86dae01b616cdac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T07:41:11.069962Z",
     "start_time": "2025-04-25T07:41:10.979557Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['寒', '随', '穷', '律', '变', '春', '逐', '鸟', '声', '开', '初', '风', '飘', '带', '柳', '晚', '雪', '间', '花', '梅', '碧', '林', '青', '旧', '竹', '绿', '沼', '翠', '新', '苔', '芝', '田', '初', '雁', '去', '绮', '树', '巧', '莺', '来']\n"
     ]
    }
   ],
   "execution_count": 32,
   "source": [
    "f = open(r'C:\\Users\\houyu\\pythonProject\\RNN文本生成\\poems_clean.txt', encoding='utf-8')\n",
    "poems = []\n",
    "for line in f.readlines():\n",
    "    title, poem = line.split(':')\n",
    "    poem = poem.replace(' ', '')\n",
    "    poem = poem.replace('\\n', '')\n",
    "    if len(poem) > 0 :\n",
    "        poems.append(list(poem))\n",
    "\n",
    "print(poems[0][:])"
   ],
   "id": "f002b9c129074cc7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "创建字符编码字典",
   "id": "a617d860de336e4b"
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2025-04-25T07:41:11.221192Z",
     "start_time": "2025-04-25T07:41:11.105097Z"
    }
   },
   "source": [
    "word2idx = {}\n",
    "i = 1\n",
    "for poem in poems:\n",
    "    for word in poem:\n",
    "        if word2idx.get(word) == None:\n",
    "            word2idx[word] = i\n",
    "            i += 1\n",
    "word2idxl[10]"
   ],
   "id": "e801fc9cb7ebe64d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'寒': 1,\n",
       " '随': 2,\n",
       " '穷': 3,\n",
       " '律': 4,\n",
       " '变': 5,\n",
       " '春': 6,\n",
       " '逐': 7,\n",
       " '鸟': 8,\n",
       " '声': 9,\n",
       " '开': 10,\n",
       " ...}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T07:41:11.257182Z",
     "start_time": "2025-04-25T07:41:11.252386Z"
    }
   },
   "source": [
    "len(word2idx)"
   ],
   "id": "b85cffafb36774de",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5545"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "对诗歌进行编码，从原始数据到矩阵",
   "id": "719d7a8d6600ed4a"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T07:41:11.643496Z",
     "start_time": "2025-04-25T07:41:11.333717Z"
    }
   },
   "source": [
    "poems_digit = []\n",
    "for poem in poems:\n",
    "    poem_digit = []\n",
    "    for word in poem:\n",
    "        poem_digit.append(word2idx[word])\n",
    "    poems_digit.append(poem_digit)\n",
    "    \n",
    "print(\"原始诗歌\")\n",
    "print(poems[3829])\n",
    "print(\"\\n 编码后的结果\")\n",
    "print(poems_digit[3829][:])"
   ],
   "id": "575536832a831201",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始诗歌\n",
      "['春', '眠', '不', '觉', '晓', '处', '处', '闻', '啼', '鸟', '夜', '来', '风', '雨', '声', '花', '落', '知', '多', '少']\n",
      "\n",
      " 编码后的结果\n",
      "[6, 2420, 57, 2468, 451, 198, 198, 747, 376, 8, 228, 39, 12, 270, 9, 19, 319, 67, 510, 1941]\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "拆分X、Y变量并处理长短不一问题",
   "id": "918c47011a2bb14c"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T07:41:11.724007Z",
     "start_time": "2025-04-25T07:41:11.660531Z"
    }
   },
   "source": [
    "# 设置诗歌最大长度为50个字符\n",
    "maxlen = 50\n",
    "X = []\n",
    "Y = []\n",
    "for poem_digit in poems_digit:\n",
    "    y=poem_digit[1:]+[0]*(maxlen - len(poem_digit))\n",
    "    Y.append(y)\n",
    "    # 将最后一个字符之前的部分作为X，并补齐字符\n",
    "    x = poem_digit[:-1] + [0]*(maxlen - len(poem_digit))\n",
    "    X.append(x)\n",
    "    \n",
    "print(\"原始诗歌\")\n",
    "print(poems[3829])\n",
    "print(\"变量X\")\n",
    "print(X[3829])\n",
    "print(\"变量Y\")\n",
    "print(Y[3829])"
   ],
   "id": "e2ad11b17577634c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始诗歌\n",
      "['春', '眠', '不', '觉', '晓', '处', '处', '闻', '啼', '鸟', '夜', '来', '风', '雨', '声', '花', '落', '知', '多', '少']\n",
      "变量X\n",
      "[6, 2420, 57, 2468, 451, 198, 198, 747, 376, 8, 228, 39, 12, 270, 9, 19, 319, 67, 510, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "变量Y\n",
      "[2420, 57, 2468, 451, 198, 198, 747, 376, 8, 228, 39, 12, 270, 9, 19, 319, 67, 510, 1941, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "划分训练集和测试集，将所有数据的顺序打乱重排",
   "id": "95b24b759081ae0e"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T07:41:11.794323Z",
     "start_time": "2025-04-25T07:41:11.750359Z"
    }
   },
   "source": [
    "idx = np.random.permutation(range(len(X)))\n",
    "X = [X[i] for i in idx]\n",
    "Y = [Y[i] for i in idx]\n",
    "\n",
    "# 切分出1/5的数据放入校验集    \n",
    "validX = X[:len(X) // 5]\n",
    "trainX = X[len(X) // 5:]\n",
    "validY = Y[:len(Y) // 5]\n",
    "trainY = Y[len(Y) // 5:]"
   ],
   "id": "664bc85d31935fba",
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-04-25T07:41:11.927729Z",
     "start_time": "2025-04-25T07:41:11.819015Z"
    }
   },
   "source": [
    "batch_size = 64\n",
    "train_ds = DataSet.TensorDataset(torch.IntTensor(np.array(trainX, dtype=int)), torch.IntTensor(np.array(trainY, dtype=int)))\n",
    "train_loader = DataSet.DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "valid_ds = DataSet.TensorDataset(torch.IntTensor(np.array(validX, dtype=int)), torch.IntTensor(np.array(validY, dtype=int)))\n",
    "valid_loader = DataSet.DataLoader(valid_ds, batch_size=batch_size, shuffle=False, num_workers=4)"
   ],
   "id": "c67bd9189c26d07e",
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-04-25T07:41:11.967248Z",
     "start_time": "2025-04-25T07:41:11.954344Z"
    }
   },
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, output_size, word_num, embedding_size, hidden_size, num_layers=1):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(word_num, embedding_size)\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "                \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        # 运算过程:先进行embedding层的计算\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x的尺寸为：batch_size，num_step，hidden_size\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        # output的尺寸为：batch_size，maxlen-1, hidden_size\n",
    "        output = self.fc(output)\n",
    "        output = output.view(-1,output.shape[-1])  #为便于后续处理，此处进行展平\n",
    "        # output的尺寸为：batch_size*(maxlen-1)，output_size\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        # 对隐含单元初始化\n",
    "        # 尺寸是layer_size，batch_size，hidden_size\n",
    "        return Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))"
   ],
   "id": "9dee6841d05e501d",
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T07:41:12.027649Z",
     "start_time": "2025-04-25T07:41:12.002812Z"
    }
   },
   "source": [
    "vocab_size = len(word2idx.keys()) + 1  # 获取文本数据集中包含的字符数量\n",
    "\n",
    "lr = 1e-3\n",
    "epochs = 50\n",
    "\n",
    "# 生成一个简单的RNN，输入size为49（50-1），输出size为vocab_size（字符总数）\n",
    "rnn = SimpleRNN(output_size=vocab_size, word_num=vocab_size, embedding_size=64, hidden_size=128)\n",
    "rnn = rnn.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
    "\n",
    "rnn"
   ],
   "id": "ceab25f641810cfc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNN(\n",
      "  (embedding): Embedding(5546, 64)\n",
      "  (rnn): RNN(64, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=5546, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T07:41:12.067187Z",
     "start_time": "2025-04-25T07:41:12.059030Z"
    }
   },
   "source": [
    "'''\n",
    "计算预测错误率的函数，pre是模型给出的一组预测结果（batch_size行、num_classes列的矩阵），label是正确标签\n",
    "'''\n",
    "def accuracy(pre, label):\n",
    "    pre = torch.max(pre.data, 1)[1] # 得到每一行（每一个样本）输出值最大元素的下标\n",
    "    rights = pre.eq(label.data).sum()\n",
    "    acc = rights.data/len(label)\n",
    "    return acc.float()"
   ],
   "id": "6fc2df3de1a94c91",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "模型验证",
   "id": "3d2302eb9a1dde75"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T07:41:12.102762Z",
     "start_time": "2025-04-25T07:41:12.092792Z"
    }
   },
   "source": [
    "def validate(model, val_loader):\n",
    "    # 在校验集上运行一遍并计算损失和准确率\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model.eval()\n",
    "    for batch, data in enumerate(val_loader):\n",
    "        init_hidden = model.initHidden(len(data[0]))\n",
    "        init_hidden = init_hidden.to(device)\n",
    "        x, y = Variable(data[0]), Variable(data[1])\n",
    "        x, y = x.to(device),y.to(device)\n",
    "        outputs, hidden = model(x, init_hidden)\n",
    "        y = y.long()\n",
    "        y = y.view(y.shape[0]*y.shape[1])\n",
    "        loss = criterion(outputs, y)\n",
    "        val_loss += loss.data.cpu().numpy()  \n",
    "        val_acc += accuracy(outputs, y)\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc /= len(val_loader)\n",
    "    return val_loss, val_acc"
   ],
   "id": "b59fa114ecd5ba15",
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T07:41:12.137892Z",
     "start_time": "2025-04-25T07:41:12.129447Z"
    }
   },
   "source": [
    "# 打印训练结果\n",
    "def print_log(epoch, train_time, train_loss, train_acc, val_loss, val_acc, epochs=10):\n",
    "    print(f\"Epoch [{epoch}/{epochs}], time: {train_time:.2f}s, loss: {train_loss:.4f}, acc: {train_acc:.4f}, val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}\")"
   ],
   "id": "4a88953272f439c0",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 定义主函数：模型训练",
   "id": "d9e844873d49fda0"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T07:41:12.178634Z",
     "start_time": "2025-04-25T07:41:12.161686Z"
    }
   },
   "source": [
    "def train(model,optimizer, train_loader, val_loader, epochs=1):\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        start = time.time()\n",
    "        for batch, data in enumerate(train_loader):\n",
    "            model.train() # 标志当前RNN处于训练阶段\n",
    "            init_hidden = model.initHidden(len(data[0])) # 初始化隐含层单元\n",
    "            init_hidden = init_hidden.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            x, y = Variable(data[0]), Variable(data[1])\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs, hidden = model(x, init_hidden) # 输入RNN，产生输出\n",
    "            y = y.long()\n",
    "            y = y.view(y.shape[0]*y.shape[1])\n",
    "            loss = criterion(outputs, y)\n",
    "            train_loss += loss.data.cpu().numpy()\n",
    "            train_acc += accuracy(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        end = time.time()\n",
    "        train_time = end - start\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc /= len(train_loader)\n",
    "        val_loss, val_acc = validate(model, val_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc) \n",
    "        print_log(epoch + 1, train_time, train_loss, train_acc, val_loss, val_acc, epochs=epochs)\n",
    "    return train_losses, train_accs, val_losses, val_accs"
   ],
   "id": "afe10670e7dff182",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 模型训练",
   "id": "deb9398d469891bc"
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-04-25T08:21:40.033926Z",
     "start_time": "2025-04-25T07:49:11.380483Z"
    }
   },
   "source": "history = train(rnn, optimizer, train_loader, valid_loader, epochs=epochs)",
   "id": "ae0617bfe9e526d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], time: 32.31s, loss: 4.2088, acc: 0.3779, val_loss: 4.2160, val_acc: 0.3836\n",
      "Epoch [2/50], time: 31.79s, loss: 4.1232, acc: 0.3823, val_loss: 4.1594, val_acc: 0.3865\n",
      "Epoch [3/50], time: 30.19s, loss: 4.0463, acc: 0.3857, val_loss: 4.1088, val_acc: 0.3893\n",
      "Epoch [4/50], time: 31.14s, loss: 3.9778, acc: 0.3888, val_loss: 4.0680, val_acc: 0.3916\n",
      "Epoch [5/50], time: 29.85s, loss: 3.9206, acc: 0.3916, val_loss: 4.0384, val_acc: 0.3937\n",
      "Epoch [6/50], time: 53.65s, loss: 3.8711, acc: 0.3947, val_loss: 4.0107, val_acc: 0.3957\n",
      "Epoch [7/50], time: 29.67s, loss: 3.8265, acc: 0.3971, val_loss: 3.9876, val_acc: 0.3974\n",
      "Epoch [8/50], time: 28.65s, loss: 3.7849, acc: 0.3999, val_loss: 3.9726, val_acc: 0.3989\n",
      "Epoch [9/50], time: 26.52s, loss: 3.7505, acc: 0.4018, val_loss: 3.9531, val_acc: 0.4008\n",
      "Epoch [10/50], time: 27.59s, loss: 3.7192, acc: 0.4035, val_loss: 3.9407, val_acc: 0.4015\n",
      "Epoch [11/50], time: 26.64s, loss: 3.6904, acc: 0.4053, val_loss: 3.9310, val_acc: 0.4022\n",
      "Epoch [12/50], time: 26.20s, loss: 3.6669, acc: 0.4067, val_loss: 3.9218, val_acc: 0.4032\n",
      "Epoch [13/50], time: 28.12s, loss: 3.6438, acc: 0.4079, val_loss: 3.9156, val_acc: 0.4038\n",
      "Epoch [14/50], time: 30.23s, loss: 3.6274, acc: 0.4090, val_loss: 3.9139, val_acc: 0.4042\n",
      "Epoch [15/50], time: 32.64s, loss: 3.6072, acc: 0.4104, val_loss: 3.9152, val_acc: 0.4043\n",
      "Epoch [16/50], time: 29.03s, loss: 3.6041, acc: 0.4105, val_loss: 3.9090, val_acc: 0.4051\n",
      "Epoch [17/50], time: 33.86s, loss: 3.5811, acc: 0.4120, val_loss: 3.9174, val_acc: 0.4047\n",
      "Epoch [18/50], time: 27.76s, loss: 3.5638, acc: 0.4131, val_loss: 3.9026, val_acc: 0.4057\n",
      "Epoch [19/50], time: 30.34s, loss: 3.5454, acc: 0.4142, val_loss: 3.9011, val_acc: 0.4061\n",
      "Epoch [20/50], time: 32.22s, loss: 3.5340, acc: 0.4151, val_loss: 3.9018, val_acc: 0.4064\n",
      "Epoch [21/50], time: 29.77s, loss: 3.5265, acc: 0.4154, val_loss: 3.9025, val_acc: 0.4065\n",
      "Epoch [22/50], time: 30.31s, loss: 3.5108, acc: 0.4165, val_loss: 3.9021, val_acc: 0.4065\n",
      "Epoch [23/50], time: 31.32s, loss: 3.4973, acc: 0.4175, val_loss: 3.9012, val_acc: 0.4068\n",
      "Epoch [24/50], time: 32.09s, loss: 3.4906, acc: 0.4181, val_loss: 3.9048, val_acc: 0.4066\n",
      "Epoch [25/50], time: 33.76s, loss: 3.4804, acc: 0.4189, val_loss: 3.9019, val_acc: 0.4073\n",
      "Epoch [26/50], time: 30.23s, loss: 3.4703, acc: 0.4197, val_loss: 3.9047, val_acc: 0.4070\n",
      "Epoch [27/50], time: 56.72s, loss: 3.4597, acc: 0.4207, val_loss: 3.9064, val_acc: 0.4074\n",
      "Epoch [28/50], time: 51.51s, loss: 3.4514, acc: 0.4209, val_loss: 3.9043, val_acc: 0.4076\n",
      "Epoch [29/50], time: 31.92s, loss: 3.4423, acc: 0.4217, val_loss: 3.9107, val_acc: 0.4074\n",
      "Epoch [30/50], time: 32.92s, loss: 3.4390, acc: 0.4222, val_loss: 3.9108, val_acc: 0.4076\n",
      "Epoch [31/50], time: 32.07s, loss: 3.4266, acc: 0.4228, val_loss: 3.9108, val_acc: 0.4077\n",
      "Epoch [32/50], time: 32.53s, loss: 3.4199, acc: 0.4236, val_loss: 3.9116, val_acc: 0.4079\n",
      "Epoch [33/50], time: 33.57s, loss: 3.4099, acc: 0.4244, val_loss: 3.9136, val_acc: 0.4078\n",
      "Epoch [34/50], time: 31.27s, loss: 3.4049, acc: 0.4249, val_loss: 3.9171, val_acc: 0.4080\n",
      "Epoch [35/50], time: 32.32s, loss: 3.4071, acc: 0.4248, val_loss: 3.9161, val_acc: 0.4079\n",
      "Epoch [36/50], time: 34.74s, loss: 3.3915, acc: 0.4259, val_loss: 3.9174, val_acc: 0.4080\n",
      "Epoch [37/50], time: 36.94s, loss: 3.3818, acc: 0.4268, val_loss: 3.9208, val_acc: 0.4080\n",
      "Epoch [38/50], time: 39.18s, loss: 3.3778, acc: 0.4272, val_loss: 3.9213, val_acc: 0.4083\n",
      "Epoch [39/50], time: 36.24s, loss: 3.3771, acc: 0.4274, val_loss: 3.9246, val_acc: 0.4079\n",
      "Epoch [40/50], time: 33.55s, loss: 3.3658, acc: 0.4283, val_loss: 3.9286, val_acc: 0.4081\n",
      "Epoch [41/50], time: 37.53s, loss: 3.3576, acc: 0.4289, val_loss: 3.9286, val_acc: 0.4081\n",
      "Epoch [42/50], time: 39.59s, loss: 3.3609, acc: 0.4287, val_loss: 4.0556, val_acc: 0.4014\n",
      "Epoch [43/50], time: 24.43s, loss: 3.3965, acc: 0.4256, val_loss: 3.9366, val_acc: 0.4077\n",
      "Epoch [44/50], time: 23.36s, loss: 3.3504, acc: 0.4295, val_loss: 3.9316, val_acc: 0.4084\n",
      "Epoch [45/50], time: 23.22s, loss: 3.3404, acc: 0.4304, val_loss: 3.9372, val_acc: 0.4084\n",
      "Epoch [46/50], time: 23.27s, loss: 3.3383, acc: 0.4305, val_loss: 3.9352, val_acc: 0.4084\n",
      "Epoch [47/50], time: 24.19s, loss: 3.3266, acc: 0.4317, val_loss: 3.9396, val_acc: 0.4086\n",
      "Epoch [48/50], time: 24.51s, loss: 3.3246, acc: 0.4321, val_loss: 3.9403, val_acc: 0.4091\n",
      "Epoch [49/50], time: 24.97s, loss: 3.3160, acc: 0.4325, val_loss: 3.9482, val_acc: 0.4081\n",
      "Epoch [50/50], time: 27.15s, loss: 3.3365, acc: 0.4308, val_loss: 3.9549, val_acc: 0.4082\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "额外项目：用RNN写藏头诗",
   "id": "3c720ec9c3a8e8e9"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T07:48:58.236055Z",
     "start_time": "2025-04-25T07:48:57.589466Z"
    }
   },
   "source": [
    "poem_incomplete = '侯****大****帅****哥****'\n",
    "poem_index = [] # 用于记录诗歌创作过程中字符和整数的对应关系\n",
    "poem_text = '' # 记录诗歌的创作过程，循环结束后应是一首完整的诗\n",
    "\n",
    "init_hidden = rnn.initHidden(1)\n",
    "init_hidden = init_hidden.to(device)\n",
    "for i in range(len(poem_incomplete)):\n",
    "    current_word = poem_incomplete[i]\n",
    "    if current_word != '*':# 若当前的字符不是\"*\"，使用word2idx字典将其变为一个整数\n",
    "        index = word2idx[current_word]\n",
    "    else:        # 若当前的字符是\"*\"，需要用RNN模型对其进行预测\n",
    "        x = poem_index + [0]*(maxlen -1 - len(poem_index))\n",
    "        x = torch.IntTensor(np.array([x], dtype=int))\n",
    "        x = Variable(x)\n",
    "        x = x.to(device)\n",
    "        pre, hidden = rnn(x, init_hidden) \n",
    "        init_hidden = hidden.data\n",
    "        crt_pre = pre[i-1].cpu() # 获取第i个时间步的输出，对应位置i-1\n",
    "        index = torch.argmax(crt_pre)\n",
    "        current_word = [k for k, v in word2idx.items() if v == index][0]\n",
    "    poem_index.append(index) \n",
    "    poem_text = poem_text + current_word"
   ],
   "id": "b4bde0596042a15d",
   "outputs": [],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T07:48:58.258129Z",
     "start_time": "2025-04-25T07:48:58.251467Z"
    }
   },
   "source": [
    "print(poem_text[0:5])\n",
    "print(poem_text[5:10])\n",
    "print(poem_text[10:15])\n",
    "print(poem_text[15:20])"
   ],
   "id": "2d94fdb0c59f47b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "侯门前路远\n",
      "大日暮雨雪\n",
      "帅人间不知\n",
      "哥水上山中\n"
     ]
    }
   ],
   "execution_count": 68
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
